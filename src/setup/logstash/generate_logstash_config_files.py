#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os

# PATH MACROS
# =============================================================================

# Repository base path
#REPO_PATH = os.path.join("..", "..", "..")

FIELDS = {
    "dataset": ["aiod_entry.date_modified", "dataset.identifier", "name", "description_identifier", "text.plain as 'plain'", "text.html as 'html'", "issn"],
    "event": ["aiod_entry.date_modified", "event.identifier", "name", "description_identifier", "text.plain", "text.html",],
    "experiment": ["aiod_entry.date_modified", "experiment.identifier", "name", "description_identifier", "text.plain as 'plain'", "text.html as 'html'"],
    "ml_model": ["aiod_entry.date_modified", "ml_model.identifier", "name", "description_identifier", "text.plain as 'plain'", "text.html as 'html'"],
    "news": ["aiod_entry.date_modified", "news.identifier", "name", "description_identifier", "text.plain as 'plain'", "text.html as 'html'", "headline",
             "alternative_headline"],
    "organisation": ["aiod_entry.date_modified", "organisation.identifier", "name", "description_identifier", "text.plain as 'plain'", "text.html as 'html'", "legal_name"],
    "project": ["aiod_entry.date_modified", "project.identifier", "name", "description_identifier", "text.plain as 'plain'", "text.html as 'html'"],
    "publication": ["aiod_entry.date_modified", "publication.identifier", "name", "description_identifier", "text.plain as 'plain'", "text.html as 'html'", "issn", "isbn"],
    "service": ["aiod_entry.date_modified", "service.identifier", "name", "description_identifier", "text.plain as 'plain'", "text.html as 'html'", "slogan"]
}

# MACROS FOR THE DOCUMENTS GENERATION FUNCTIONS
# =============================================================================

INFO = """{0} This file has been generated by `logstash_config.py` file
{0} ---------------------------------------------------------
"""

CONF_BASE = """http.host: "0.0.0.0"
xpack.monitoring.elasticsearch.hosts: [ "http://elasticsearch:9200" ]
xpack.monitoring.enabled: true
xpack.monitoring.elasticsearch.username: {0}
xpack.monitoring.elasticsearch.password: {1}
"""

INIT_INPUT_BASE = """  jdbc {{
    jdbc_driver_library => "/usr/share/logstash/mysql-connector-j.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://sqlserver:3306/aiod"
    jdbc_user => "{0}"
    jdbc_password => "{1}"
    clean_run => true
    record_last_run => false
    statement_filepath => "/usr/share/logstash/sql/init_{2}.sql"
    type => "{2}"
  }}
"""

SYNC_INPUT_BASE = """  jdbc {{
    jdbc_driver_library => "/usr/share/logstash/mysql-connector-j.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://sqlserver:3306/aiod"
    jdbc_user => "{0}"
    jdbc_password => "{1}"
    use_column_value => true
    tracking_column => "date_modified"
    tracking_column_type => "timestamp"
    schedule => "*/5 * * * * *"
    statement_filepath => "/usr/share/logstash/sql/sync_{2}.sql"
    type => "{2}"
  }}
  jdbc {{
    jdbc_driver_library => "/usr/share/logstash/mysql-connector-j.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://sqlserver:3306/aiod"
    jdbc_user => "{0}"
    jdbc_password => "{1}"
    use_column_value => true
    tracking_column => "date_deleted"
    tracking_column_type => "timestamp"
    schedule => "*/5 * * * * *"
    statement_filepath => "/usr/share/logstash/sql/rm_{2}.sql"
    type => "rm_{2}"
  }}
"""

FILTER_BASE = """filter {{
  if ![application_area] {{
    mutate {{
      replace => {{"application_area" => ""}}
    }}
  }}
  mutate {{
    remove_field => ["@version", "@timestamp"]
    split => {{"application_area" => ","}}
  }}{0}
}}
"""

DATE_FILTER = """
  if [type] == "organisation" {0}{{
      ruby {{
        code => '
            t = Time.at(event.get("date_founded").to_f)
            event.set("date_founded", t.strftime("%Y-%m-%d"))
        '
      }}
  }}
"""

SYNC_DATE_FILTER_ADDON = """or [type] == "rm_organisation" """

INIT_OUTPUT_BASE = """  if [type] == "{2}" {{
    elasticsearch {{
        hosts => "elasticsearch:9200"
        user => "{0}"
        password => "{1}"
        ecs_compatibility => disabled
        index => "{2}"
        document_id => "{2}_%{{identifier}}"
    }}
  }}
"""

SYNC_OUTPUT_BASE = """  if [type] == "{2}" {{
    elasticsearch {{
        hosts => "elasticsearch:9200"
        user => "{0}"
        password => "{1}"
        ecs_compatibility => disabled
        index => "{2}"
        document_id => "{2}_%{{identifier}}"
    }}
  }}
  if [type] == "rm_{2}" {{
    elasticsearch {{
        action => "delete"
        hosts => "elasticsearch:9200"
        user => "{0}"
        password => "{1}"
        ecs_compatibility => disabled
        index => "{2}"
        document_id => "{2}_%{{identifier}}"
    }}
  }}
"""

SQL_BASE = """SELECT {1}
FROM aiod.{0}
INNER JOIN aiod.aiod_entry ON aiod.{0}.aiod_entry_identifier=aiod.aiod_entry.identifier
LEFT JOIN aiod.text ON aiod.{0}.description_identifier=aiod.text.identifier{2}
"""

SQL_RM_BASE = """SELECT {0}.identifier
FROM aiod.{0}
WHERE aiod.{0}.date_deleted IS NOT NULL AND aiod.{0}.date_deleted > :sql_last_value
"""

INIT_CLAUSE = """
WHERE aiod.{0}.date_deleted IS NULL"""

SYNC_CLAUSE = """
WHERE aiod.{0}.date_deleted IS NULL AND aiod.aiod_entry.date_modified > :sql_last_value"""

# DOCUMENTS GENERATION FUNCTIONS
# =============================================================================

def generate_conf_file(conf_path, es_user, es_pass):
    
    file_path = os.path.join(conf_path, "logstash.yml")
    
    # Generate configuration file
    with open(file_path, 'w') as f:
        
        # Info
        f.write(INFO.format('#'))
        
        # Configuration
        f.write(CONF_BASE.format(es_user, es_pass))

def generate_pipeline_conf_files(pipeline_conf_path, db_user, db_pass,
                                 es_user, es_pass, entities, sync=False):
    
    if not sync: # init file
        file_path = os.path.join(pipeline_conf_path, "init_table.conf")
        input_base = INIT_INPUT_BASE
        date_filter = DATE_FILTER.format("")
        output_base = INIT_OUTPUT_BASE
    else: # sync file
        file_path = os.path.join(pipeline_conf_path, "sync_table.conf")
        input_base = SYNC_INPUT_BASE
        date_filter = DATE_FILTER.format(SYNC_DATE_FILTER_ADDON)
        output_base = SYNC_OUTPUT_BASE
    
    # Generate configuration file
    with open(file_path, 'w') as f:
        
        # Info
        f.write(INFO.format('#'))
        
        # Input
        f.write("input {\n")
        for entity in entities:
            f.write(input_base.format(db_user, db_pass, entity))
        f.write("}\n")
        
        # Filters
        if "organisation" in entities:
            f.write(FILTER_BASE.format(date_filter))
        else:
            f.write(FILTER_BASE.format(""))
        
        # Output
        f.write("output {\n")
        for entity in entities:
            f.write(output_base.format(es_user, es_pass, entity))
        f.write("}\n")

def generate_pipeline_sql_files(pipeline_sql_path, entity, sync=False):
    
    # Generate output file path
    if sync:
        file_path = os.path.join(pipeline_sql_path, f"sync_{entity}.sql")
    else:
        file_path = os.path.join(pipeline_sql_path, f"init_{entity}.sql")
    
    # Write the output file
    with open(file_path, 'w') as f:
        
        # Info
        f.write(INFO.format('--'))
        
        # Where clause
        if sync:
            where_clause = SYNC_CLAUSE.format(entity)
        else:
            where_clause = INIT_CLAUSE.format(entity)
        
        f.write(SQL_BASE.format(entity, ", ".join(FIELDS[entity]), where_clause))

def generate_pipeline_sql_rm_files(pipeline_sql_path, entity):
    
    # Generate output file path
    file_path = os.path.join(pipeline_sql_path, f"rm_{entity}.sql")
    
    # Write the output file
    with open(file_path, 'w') as f:
        
        # Info
        f.write(INFO.format('--'))
        
        # SQL query
        f.write(SQL_RM_BASE.format(entity))

# MAIN FUNCTION
# =============================================================================

def main(base_path, db_user, db_pass, es_user, es_pass, entities,
         ai_asset_entities, attributes, type_entities, mode_entities,
         status_entities, agent_entities, organisation_entities):
    
    # Make configuration dir
    conf_path = os.path.join(base_path, "config")
    os.makedirs(conf_path, exist_ok=True)
    
    # Make pipeline configuration dirs
    pipeline_conf_path = os.path.join(base_path, "pipeline", "conf")
    os.makedirs(pipeline_conf_path, exist_ok=True)
    pipeline_sql_path = os.path.join(base_path, "pipeline", "sql")
    os.makedirs(pipeline_sql_path, exist_ok=True)
    
    # Generate logstash configuration file
    generate_conf_file(conf_path, es_user, es_pass)
    
    # Generate pipeline configuration init file
    generate_pipeline_conf_files(pipeline_conf_path, db_user, db_pass,
                                 es_user, es_pass, entities, sync=False)
    
    # Generate pipeline configuration sync file
    generate_pipeline_conf_files(pipeline_conf_path, db_user, db_pass,
                                 es_user, es_pass, entities, sync=True)
    
    # Generate SQL init and sync files
    for entity in entities:
        generate_pipeline_sql_files(pipeline_sql_path, entity, sync=False)
        generate_pipeline_sql_files(pipeline_sql_path, entity, sync=True)
    
     # Generate SQL rm files
    for entity in entities:
        generate_pipeline_sql_rm_files(pipeline_sql_path, entity)

if __name__ == "__main__":
    
    # PATH MACROS
    # -------------------------------------------------------------------------
    
    # Repository base path
#    repo_path = REPO_PATH
    
    # Configuration base path
#    base_path = os.path.join(repo_path, "logstash")
    base_path = "/logstash"
    
    # -------------------------------------------------------------------------
    
    # Users and passwords
    db_user = "root"
    db_pass = os.environ['MYSQL_ROOT_PASSWORD']
    es_user = os.environ['ES_USER']
    es_pass = os.environ['ES_PASSWORD']
#    with open(os.path.join(repo_path, ".env"), "r") as f:
#        for line in f:
#            if "MYSQL_ROOT_PASSWORD" in line:
#                db_pass = line.split("=")[1][:-1]
#            if "ES_USER" in line:
#                es_user = line.split("=")[1][:-1]
#            if "ES_PASSWORD" in line:
#                es_pass = line.split("=")[1][:-1]
    
    # Entities and attributes
    entities = ["dataset", "event", "experiment", "ml_model", "news",
                "organisation", "project", "publication", "service"]
    ai_asset_entities = ["dataset", "experiment", "ml_model", "publication"]
    attributes = {
        "dataset": ["issn", "measurement_technique", "temporal_coverage"],
        "event": ["start_date", "end_date", "schedule", "registration_link",
                  "organiser_identifier"],
        "experiment": ["experimental_workflow", "execution_settings",
                       "reproducibility_explanation"],
        "news": ["headline", "alternative_headline"],
        "organisation": ["date_founded", "legal_name"],
        "project": ["start_date", "end_date", "total_cost_euro",
                    "coordinator_identifier"],
        "publication": ["permanent_identifier", "isbn", "issn",
                        "knowledge_asset_id AS `knowledge_asset_identifier`"],
        "service": ["slogan", "terms_of_service"]
    }
    type_entities = ["ml_model", "organisation", "publication"]
    mode_entities = ["event"]
    status_entities = ["event"]
    agent_entities = {
        "event": ("organiser_identifier", "organiser_type"),
        "organisation": ("agent_id", "agent")
    }
    organisation_entities = {
        "project": ("coordinator_identifier", "coordinator_name")
    }
    
    # Main function
    main(base_path, db_user, db_pass, es_user, es_pass, entities,
         ai_asset_entities, attributes, type_entities, mode_entities,
         status_entities, agent_entities, organisation_entities)
